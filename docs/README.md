# FusionQA

## ğŸš€ Project Overview
FusionQA is a **hybrid Retrieval-Augmented Generation (RAG) system** that combines **vector search** and **graph search** for question answering. It enables flexible querying from PDF documents by generating both a **Neo4j knowledge graph** and a **FAISS vector database**, and fusing their contexts using LLMs.

### [ğŸ”— Summarizer API documentation link](https://tuhindutta.github.io/FusionQA/api_doc.html)
### [ğŸ”— Docker Hub image and user guide](https://hub.docker.com/r/tkdutta/fusion-qa)

---
FusionQA provides:

* **Vector Search** â†’ Semantic search using embeddings.
* **Graph Search** â†’ Querying structured knowledge in a Neo4j graph.
* **Hybrid Search** â†’ Fuses vector + graph contexts for richer answers.
* **Configurable Roles & Filters** â†’ System roles (e.g., tutor, banking assistant) and graph node/relationship filtering.

It is designed for **flexibility** (plugging in prebuilt vector DBs/graphs), **modularity** (separate sub-packages), and **scalability** (API-driven via FastAPI + Docker).

---

## ğŸ“‚ Directory Structure

```
summarizer
â”‚   app.py                # Main FastAPI app
â”‚   generate_graph.py     # Script to generate graph from documents
â”‚   generate_vector.py    # Script to generate vector DB
â”‚
â”œâ”€â”€â”€data_processing       # Document loading & preprocessing
â”‚       loader.py
â”‚       utils.py
â”‚
â”œâ”€â”€â”€documents             # PDF files to process
â”‚
â”œâ”€â”€â”€graph                 # Neo4j graph-related functionality
â”‚       prepare.py
â”‚       token_match.py
â”‚
â”œâ”€â”€â”€llm                    # LLM modules
â”‚       base_llm.py
â”‚       embedding.py       # Chunking, embeddings, similarity search
â”‚       graph_llm.py       # Graph-based reasoning
â”‚       hybrid_llm.py      # Fusion of graph + vector contexts
â”‚       vector_llm.py      # Vector-based reasoning
â”‚
â”œâ”€â”€â”€vector_db              # Vector DB storage (FAISS + metadata)
â””â”€â”€â”€vector_store           # Sample/placeholder vector store
        index.faiss
        index.pkl
```

---

## ğŸ§© Sub-Packages & Modules

### 1. `data_processing`

* Loads documents from `documents/`.
* Cleans and preprocesses content before further processing.

### 2. `llm`

* **`base_llm.py`** â†’ Base class for LLMs, extended by other modules.
* **`embedding.py`** â†’ Splits documents into chunks, creates embeddings, stores in vector DB, performs similarity searches.
* **`vector_llm.py`** â†’ Vector-based retrieval + response generation.
* **`graph_llm.py`** â†’ Uses `graph.prepare.Graph` to retrieve graph context.
* **`hybrid_llm.py`** â†’ Combines vector and graph contexts for fused reasoning.

### 3. `graph`

* Connects to **Neo4j graph DB**.
* Retrieves schema, nodes, and relationships.
* Matches query input to graph elements.
* Provides graph generation utilities.

### 4. `vector_store`

* Contains a **sample FAISS-based vector store**.
* Used if no external DB is available.

### 5. `vector_db`

* Stores generated FAISS index and metadata from documents.

### 6. Scripts

* **`generate_graph.py`** â†’ Builds graph from documents.
* **`generate_vector.py`** â†’ Builds vector DB from documents.
* **`app.py`** â†’ Launches the FastAPI app, exposes API.

---

## ğŸ” LLMs & Models Used

* **Graph generation, graph/vector responses, node/relationship retrieval** â†’ `llama-3.3-70b-versatile`
* **Cypher query generation** â†’ `openai/gpt-oss-120b`
* **Semantic embeddings for RAG** â†’ `sentence-transformers/all-mpnet-base-v2`

---

## ğŸ”„ Workflow

1. Place clean PDF documents inside `documents/`.
2. Generate **graph** & **vector DB** using API endpoints.
3. Refresh the vector DB using the refresh endpoint.
4. *(Optional)* Place ready-made vector DB files directly in `vector_db/` and refresh, skipping steps 1â€“2. Graph can also be created independently.
5. *(Optional)* Set **system role** (e.g., banking assistant, tutor).
6. *(Optional)* Restrict **node/relationship types** for graph queries.
7. Perform queries â†’ vector search, graph search, or hybrid.

---

## ğŸ“¦ Major Dependencies

* **FastAPI** â†’ API framework (`app.py`).
* **Uvicorn** â†’ ASGI server.
* **FAISS** â†’ Vector similarity search engine.
* **Neo4j** â†’ Graph database backend.
* **LangChain** â†’ Orchestrates LLM workflows.
* **Sentence-Transformers** â†’ Embedding model for RAG.
* **Transformers + Torch** â†’ LLM + embeddings support.
* **pypdf** â†’ PDF parsing.
* **Pydantic** â†’ Data validation.

(See `requirements.txt` for full list.)

---

## ğŸ³ Docker Setup

### Dockerfile

```dockerfile
FROM python:3.11.13-slim

WORKDIR /app

COPY requirements.txt .

RUN python -m pip install --upgrade pip && \
    python -m pip install --no-cache-dir -r requirements.txt

COPY summarizer/ ./summarizer

WORKDIR /app/summarizer

EXPOSE 8000

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Environment File

Create a `.env` file in the project root:

```env
NEO4J_URI=example
NEO4J_USERNAME=example
NEO4J_PASSWORD=example
GROQ_API_KEY=example
```

### Run the Container

```bash
docker run --env-file ./.env \
  -v ${PWD}/documents:/app/summarizer/documents \
  -v ${PWD}/vector_db:/app/summarizer/vector_db \
  -p 8000:8000 summarizer_app
```

**Options**:

* `--env-file ./.env` â†’ Load env variables.
* `-v ${PWD}/documents:/app/summarizer/documents` â†’ Mount documents directory.
* `-v ${PWD}/vector_db:/app/summarizer/vector_db` â†’ Persist vector DB.
* `-p 8000:8000` â†’ API accessible at port 8000.

### API Access

* Swagger UI â†’ [http://localhost:8000/docs](http://localhost:8000/docs)
* ReDoc â†’ [http://localhost:8000/redoc](http://localhost:8000/redoc)

---

## ğŸ“– API Documentation

Full API reference is available separately at:
ğŸ‘‰ [FusionQA API Docs](https://tuhindutta.github.io/FusionQA/api_doc.html)

---

## âœ¨ Summary

FusionQA is a **modular, LLM-powered, hybrid RAG framework** that:

* Ingests and preprocesses PDFs.
* Builds both vector and graph representations.
* Supports standalone or fused querying.
* Exposes functionality via FastAPI.
* Deployable locally or via Docker.

This makes it suitable for domains like **finance, education, research, and knowledge management**, where combining **semantic search** with **structured graph reasoning** provides superior results.

