# FusionQA

## ğŸš€ Project Overview
FusionQA is a **hybrid Retrieval-Augmented Generation (RAG) system** that combines **vector search** and **graph search** for question answering. It enables flexible querying from PDF documents by generating both a **Neo4j knowledge graph** and a **FAISS vector database**, and fusing their contexts using LLMs.

### [ğŸ”— Summarizer API documentation link](https://tuhindutta.github.io/FusionQA/api_doc.html)
### [ğŸ”— Docker Hub image and user guide](https://hub.docker.com/r/tkdutta/fusion-qa)

---
FusionQA provides:

* **Vector Search** â†’ Semantic search using embeddings.
* **Graph Search** â†’ Querying structured knowledge in a Neo4j graph.
* **Hybrid Search** â†’ Fuses vector + graph contexts for richer answers.
* **Configurable Roles & Filters** â†’ System roles (e.g., tutor, banking assistant) and graph node/relationship filtering.

It is designed for **flexibility** (plugging in prebuilt vector DBs/graphs), **modularity** (separate sub-packages), and **scalability** (API-driven via FastAPI + Docker).

---

## ğŸ“‚ Directory Structure

```
summarizer
â”‚   app.py                # Main FastAPI app
â”‚   generate_graph.py     # Script to generate graph from documents
â”‚   generate_vector.py    # Script to generate vector DB
â”‚
â”œâ”€â”€â”€data_processing       # Document loading & preprocessing
â”‚       loader.py
â”‚       utils.py
â”‚
â”œâ”€â”€â”€documents             # PDF files to process
â”‚
â”œâ”€â”€â”€graph                 # Neo4j graph-related functionality
â”‚       prepare.py
â”‚       token_match.py
â”‚
â”œâ”€â”€â”€llm                    # LLM modules
â”‚       base_llm.py
â”‚       embedding.py       # Chunking, embeddings, similarity search
â”‚       graph_llm.py       # Graph-based reasoning
â”‚       hybrid_llm.py      # Fusion of graph + vector contexts
â”‚       vector_llm.py      # Vector-based reasoning
â”‚
â”œâ”€â”€â”€vector_db              # Vector DB storage (FAISS + metadata)
â””â”€â”€â”€vector_store           # Sample/placeholder vector store
        index.faiss
        index.pkl
```

---

## ğŸ§© Sub-Packages & Modules

### 1. `data_processing`

* Loads documents from `documents/`.
* Cleans and preprocesses content before further processing.

### 2. `llm`

* **`base_llm.py`** â†’ Base class for LLMs, extended by other modules.
* **`embedding.py`** â†’ Splits documents into chunks, creates embeddings, stores in vector DB, performs similarity searches.
* **`vector_llm.py`** â†’ Vector-based retrieval + response generation.
* **`graph_llm.py`** â†’ Uses `graph.prepare.Graph` to retrieve graph context.
* **`hybrid_llm.py`** â†’ Combines vector and graph contexts for fused reasoning.

### 3. `graph`

* Connects to **Neo4j graph DB**.
* Retrieves schema, nodes, and relationships.
* Matches query input to graph elements.
* Provides graph generation utilities.

### 4. `vector_store`

* Contains a **sample FAISS-based vector store**.
* Used if no external DB is available.

### 5. `vector_db`

* Stores generated FAISS index and metadata from documents.

### 6. Scripts

* **`generate_graph.py`** â†’ Builds graph from documents.
* **`generate_vector.py`** â†’ Builds vector DB from documents.
* **`app.py`** â†’ Launches the FastAPI app, exposes API.

---

## ğŸ” LLMs & Models Used

* **Graph generation, graph/vector responses, node/relationship retrieval** â†’ `llama-3.3-70b-versatile`
* **Cypher query generation** â†’ `openai/gpt-oss-120b`
* **Semantic embeddings for RAG** â†’ `sentence-transformers/all-mpnet-base-v2`

---

## ğŸ”„ Workflow

1. Place clean PDF documents inside `documents/`.
2. Generate **graph** & **vector DB** using API endpoints.
3. Refresh the vector DB using the refresh endpoint.
4. *(Optional)* Place ready-made vector DB files directly in `vector_db/` and refresh, skipping steps 1â€“2. Graph can also be created independently.
5. *(Optional)* Set **system role** (e.g., banking assistant, tutor).
6. *(Optional)* Restrict **node/relationship types** for graph queries.
7. Perform queries â†’ vector search, graph search, or hybrid.

---

## ğŸ“¦ Major Dependencies

* **FastAPI** â†’ API framework (`app.py`).
* **Uvicorn** â†’ ASGI server.
* **FAISS** â†’ Vector similarity search engine.
* **Neo4j** â†’ Graph database backend.
* **LangChain** â†’ Orchestrates LLM workflows.
* **Sentence-Transformers** â†’ Embedding model for RAG.
* **Transformers + Torch** â†’ LLM + embeddings support.
* **pypdf** â†’ PDF parsing.
* **Pydantic** â†’ Data validation.

(See `requirements.txt` for full list.)

---

## âœ¨ Summary

FusionQA is a **modular, LLM-powered, hybrid RAG framework** that:

* Ingests and preprocesses PDFs.
* Builds both vector and graph representations.
* Supports standalone or fused querying.
* Exposes functionality via FastAPI.
* Deployable locally or via Docker.

This makes it suitable for domains like **finance, education, research, and knowledge management**, where combining **semantic search** with **structured graph reasoning** provides superior results.

---

## ğŸ¤ Contributing
Contributions are welcome! Please fork the repository and submit a pull request for any enhancements or bug fixes. For more details and updates, visit the [GitHub Repository](https://github.com/tuhindutta/FusionQA).
